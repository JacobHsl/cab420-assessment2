{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eba78b-78ab-487d-b63d-f127ac7e82f9",
   "metadata": {
    "id": "87eba78b-78ab-487d-b63d-f127ac7e82f9"
   },
   "source": [
    "# CAB_420-A2: Enron Email Sender Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943258d-889d-48a1-9df0-1ee98bda2c92",
   "metadata": {
    "id": "1943258d-889d-48a1-9df0-1ee98bda2c92"
   },
   "source": [
    "# 1. Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577e02ce-521b-4689-aca2-03c8afed9912",
   "metadata": {
    "id": "577e02ce-521b-4689-aca2-03c8afed9912",
    "outputId": "77c33816-e08e-4e79-ab1f-40475f7a8331"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 14:36:08.291308: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-18 14:36:08.455618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747542968.514791  638606 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747542968.532424  638606 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747542968.678428  638606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747542968.678448  638606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747542968.678449  638606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747542968.678450  638606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 14:36:08.694063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# to set up the venv, run\n",
    "# pip install keras matplotlib seaborn scikit-learn scikit-image statsmodels opencv-python pandas gensim pillow tensorflow-datasets xgboost jupyter ipython jupyterlab openpyxl\n",
    "\n",
    "\n",
    "# Imports\n",
    "import tarfile\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53d942e-6077-496f-9d93-938432ca469c",
   "metadata": {
    "id": "b53d942e-6077-496f-9d93-938432ca469c"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "tar_path = 'enron_mail_20150507.tar.gz'\n",
    "data_path = 'maildir'\n",
    "Email = namedtuple('Email', ['from_', 'message'])\n",
    "max_emails = 500 # Senders will stop having emails appended to their dataset once this number is reached\n",
    "min_emails = 200 # Minimum emails a class must have to be kept\n",
    "min_words = 20 # Minimum words an email needs to have to be kept\n",
    "# max_words = 200 # Max words an email needs to have to be kept\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Functions\n",
    "# Decrypts tar file\n",
    "def decrypt_tar(file_path):\n",
    "    # Decrypt tar.zg file\n",
    "    if not os.path.isdir(data_path):\n",
    "        try:\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                members = tar.getmembers()\n",
    "                for member in tqdm(members, desc=\"Extracting files\"):\n",
    "                    tar.extract(member)\n",
    "\n",
    "           # delete archive after extraction\n",
    "           # os.remove(file_path)\n",
    "           # print(f\"Deleted archive: {file_path}\")\n",
    "        except:\n",
    "            print(f\"File '{file_path}' does not exist.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Folder '{data_path}' already exists.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Adapted from https://www.youtube.com/watch?v=_hkmSGG9pyA (Pythonic Accountant on YouTube)\n",
    "# Takes all individual email files and returns them in Email objects that have referenceable sections\n",
    "def get_msg(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            from_ = ''\n",
    "            message = []\n",
    "            in_message = False\n",
    "\n",
    "            content = f.read().replace('\\r', '')\n",
    "            lines = content.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith('From:') and not from_:\n",
    "                    from_ = line.replace('From:', '').strip()\n",
    "                elif line.startswith('X-FileName:'):\n",
    "                    in_message = True\n",
    "                elif in_message:\n",
    "                    # Skip quoted replies (email forwards)\n",
    "                    if line.strip().lower().startswith('-----Original Message-----'):\n",
    "                        break\n",
    "                    message.append(line)\n",
    "\n",
    "            return Email(\n",
    "                from_=from_,\n",
    "                message=' '.join(message).strip()\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Filters out all classes with less than the minimum samples\n",
    "def filter_classes(x, y):\n",
    "    # First pass: apply filters\n",
    "    sender_counts = Counter(y)\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "\n",
    "    for xi, yi in zip(x, y):\n",
    "        xj = preprocess(xi)\n",
    "        if (\n",
    "            sender_counts[yi] >= min_emails and\n",
    "            len(xj.split()) > min_words and\n",
    "            'enron' in yi.lower()\n",
    "        ):\n",
    "            x_temp.append(xj)\n",
    "            y_temp.append(yi)\n",
    "\n",
    "    # Second pass: drop entries that dont fit the min-max_email thresholds\n",
    "    filtered_counts = Counter(y_temp)\n",
    "    x_filtered = []\n",
    "    y_filtered = []\n",
    "\n",
    "    for xi, yi in zip(x_temp, y_temp):\n",
    "        if filtered_counts[yi] >= min_emails and filtered_counts[yi] <= max_emails:\n",
    "            x_filtered.append(xi)\n",
    "            y_filtered.append(yi)\n",
    "\n",
    "    count_filtered = Counter(y_filtered)\n",
    "    print(f\"Filtered: {len(x_filtered)} emails from {len(set(y_filtered))} users\")\n",
    "    return x_filtered, y_filtered, count_filtered\n",
    "\n",
    "\n",
    "# Extracts all emails from the maildir file.\n",
    "def load_directory(maildir_path):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    #list of all folders commonly found in users emails that contain non-useful informaton.\n",
    "    irrelevant_folders = {'calendar', 'contacts', 'notes', 'deleted_items', 'journal'}\n",
    "    \n",
    "    for root, dirs, files in os.walk(maildir_path):\n",
    "        if any(folder in root.lower() for folder in irrelevant_folders): #This skips folders matching the names listed in the 'irrelevant_folders' variable above.\n",
    "            continue\n",
    "        for email_file in files:\n",
    "            file_path = os.path.join(root, email_file)\n",
    "\n",
    "            # Skip directories (until a file is hit)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            # Open file, parse contents, and add to x/y arrays\n",
    "            try:\n",
    "                email = get_msg(file_path)\n",
    "                # Only keep emails that have a from and message\n",
    "                if email and email.from_ and email.message:\n",
    "                    x.append(email.message)\n",
    "                    y.append(email.from_)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    count = Counter(y)\n",
    "    print('Unfiltered data loaded')\n",
    "    return(x, y, count)\n",
    "\n",
    "def hash_text(text):    \n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Process email body data\n",
    "def preprocess(text):\n",
    "\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Strip out email addresses and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove common attachment artefacts\n",
    "    text = re.sub(r'(?i)\\b[\\w\\-.]+\\.(jpg|jpeg|png|pdf|docx?|xlsx?|pptx?|zip)\\b', '', text)\n",
    "    text = re.sub(r'<<[^>]+>>', '', text)\n",
    "    text = re.sub(r'\\[cid:[^\\]]+\\]', '', text)\n",
    "\n",
    "    # Remove inline headers (leaked from replies)\n",
    "    text = re.sub(r'(?i)(from|to|cc|subject|sent):\\s?.*', '', text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d5f30-56b6-45e0-bef9-0c3af82608af",
   "metadata": {
    "id": "657d5f30-56b6-45e0-bef9-0c3af82608af"
   },
   "source": [
    "# 2. Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb51ec3-d6a5-484a-a2df-0ebfeaf72cd5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4375777d2e994faca01c30d76d154946"
     ]
    },
    "id": "2eb51ec3-d6a5-484a-a2df-0ebfeaf72cd5",
    "outputId": "15c84ef1-7577-48dc-83b6-d864703b5db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'maildir' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Extract tar file\n",
    "decrypt_tar(tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323959cf-f247-48db-bf1b-b8205324517e",
   "metadata": {
    "id": "323959cf-f247-48db-bf1b-b8205324517e",
    "outputId": "5cd4d567-a983-4779-a479-8f2a3b5aadc5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered data loaded\n",
      "Filtered: 26339 emails from 86 users\n",
      "Deduplicated: 13181 emails from 86 users\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "# This section may take a while, wait for print below\n",
    "x, y, count = load_directory(data_path)\n",
    "x_filtered, y_filtered, count_filtered = filter_classes(x, y)\n",
    "\n",
    "seen = set()\n",
    "dedup_x, dedup_y = [], []\n",
    "\n",
    "for xi, yi in zip(x_filtered, y_filtered):\n",
    "    h = hash_text(xi)\n",
    "    if h not in seen:\n",
    "        seen.add(h)\n",
    "        dedup_x.append(xi)\n",
    "        dedup_y.append(yi)\n",
    "\n",
    "print(f\"Deduplicated: {len(dedup_x)} emails from {len(set(dedup_y))} users\")\n",
    "\n",
    "x_filtered = dedup_x\n",
    "y_filtered = dedup_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f511595-f28b-49aa-aaee-5e087a80afc5",
   "metadata": {
    "id": "d47c4bb9-8c6e-41dd-8386-35f9e70ef757",
    "outputId": "74b22731-2d14-4009-c832-d7f341d895c3"
   },
   "outputs": [],
   "source": [
    "# Analysis of filtered vs unfiltered data\n",
    "print(f\"Unfiltered: {len(x)} emails from {len(set(y))} users\")\n",
    "print(f\"Filtered:   {len(x_filtered)} emails from {len(set(y_filtered))} users with {min_emails}-{max_emails} emails\")\n",
    "\n",
    "top_threshold = 1000 # Lower = faster charts, showing less data\n",
    "# Print top filtered senders\n",
    "if top_threshold > len(set(y_filtered)):\n",
    "    print(f\"\\nFiltered senders ({len(set(y_filtered))}):\")\n",
    "else:\n",
    "    print(f\"\\nFiltered senders ({top_threshold}):\")\n",
    "for sender, email_count in count_filtered.most_common(top_threshold):\n",
    "    print(f\"{sender}: {email_count} emails\")\n",
    "\n",
    "# Extract top data for plotting\n",
    "top_unfiltered = count.most_common(top_threshold)\n",
    "top_filtered = count_filtered.most_common(top_threshold)\n",
    "\n",
    "# Separate into sender and count lists\n",
    "senders_unf, counts_unf = zip(*top_unfiltered)\n",
    "senders_filt, counts_filt = zip(*top_filtered)\n",
    "\n",
    "# Plot data\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 16), sharey=True)\n",
    "# Unfiltered plot\n",
    "ax1.bar(senders_unf[::-1], counts_unf[::-1], color='salmon')\n",
    "ax1.set_title('Top Senders (Unfiltered)')\n",
    "ax1.set_ylabel('Email Count')\n",
    "ax1.set_xticklabels([])\n",
    "# Filtered plot\n",
    "ax2.bar(senders_filt[::-1], counts_filt[::-1], color='seagreen')\n",
    "ax2.set_title('Top Senders (Filtered)')\n",
    "ax2.set_ylabel('Email Count')\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6271-7ab8-47e6-af19-c04a30ffba63",
   "metadata": {
    "id": "52de6271-7ab8-47e6-af19-c04a30ffba63",
    "outputId": "f22785d7-642f-47c2-fc6a-d0f084366f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 28290\n",
      "x_test: 9431\n",
      "x_val: 9430\n"
     ]
    }
   ],
   "source": [
    "# Encode y (senders)\n",
    "y_encoded = le.fit_transform(y_filtered)\n",
    "\n",
    "# Split Data\n",
    "# Split (train+val) vs test\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_filtered, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Split train vs val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "print(f\"x_val: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c3ec6-95a2-45e8-a45c-ec98743e4264",
   "metadata": {
    "id": "789c3ec6-95a2-45e8-a45c-ec98743e4264"
   },
   "source": [
    "## 2.1. Processing for Non-DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7403fde-5d9a-4e51-84a6-54892cad2b44",
   "metadata": {
    "id": "f7403fde-5d9a-4e51-84a6-54892cad2b44",
    "outputId": "2d7921bd-08eb-4410-f03a-e9b304484530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (28290, 88217)\n",
      "X_test: (9431, 88217)\n",
      "X_val: (9430, 88217)\n"
     ]
    }
   ],
   "source": [
    "# Extra processing for non-DL models\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_val   = vectorizer.transform(x_val)\n",
    "X_test  = vectorizer.transform(x_test)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22afb2-d8ef-44b1-b25c-0ef1f0bb864b",
   "metadata": {
    "id": "3b22afb2-d8ef-44b1-b25c-0ef1f0bb864b"
   },
   "source": [
    "## 2.2. Processing for DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995f667-ed55-4514-8e8a-3a333d89b341",
   "metadata": {
    "id": "c995f667-ed55-4514-8e8a-3a333d89b341",
    "outputId": "9d4ff8f4-835c-4bd7-e012-65db75324ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_seq: 28290\n",
      "X_test_seq: 9431\n",
      "X_val_seq: 9430\n",
      "X_train_pad: 28290\n",
      "X_test_pad: 9431\n",
      "X_val_pad: 9430\n"
     ]
    }
   ],
   "source": [
    "# Extra processing for DL models\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\", lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(x_test)\n",
    "X_val_seq   = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=300, padding='post')\n",
    "X_test_pad  = pad_sequences(X_test_seq, maxlen=300, padding='post')\n",
    "X_val_pad   = pad_sequences(X_val_seq, maxlen=300, padding='post')\n",
    "\n",
    "print(f\"X_train_seq: {len(X_train_seq)}\")\n",
    "print(f\"X_test_seq: {len(X_test_seq)}\")\n",
    "print(f\"X_val_seq: {len(X_val_seq)}\")\n",
    "print(f\"X_train_pad: {len(X_train_pad)}\")\n",
    "print(f\"X_test_pad: {len(X_test_pad)}\")\n",
    "print(f\"X_val_pad: {len(X_val_pad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311c115-8152-4a31-83cb-2e4dce56a2f0",
   "metadata": {
    "id": "6311c115-8152-4a31-83cb-2e4dce56a2f0"
   },
   "source": [
    "# 3. ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f85cf5-0102-4f00-aba9-159d23daf106",
   "metadata": {
    "id": "b0f85cf5-0102-4f00-aba9-159d23daf106"
   },
   "source": [
    "## 3.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a8c32-9b31-4a10-9541-7c6a3e5b2b31",
   "metadata": {
    "id": "3c0a8c32-9b31-4a10-9541-7c6a3e5b2b31"
   },
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "n = 100\n",
    "r = 42\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators = n, random_state = r)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "train_preds = rf_model.predict(X_train)\n",
    "val_preds = rf_model.predict(X_val)\n",
    "test_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print('model fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a873f637-69ea-404a-b705-035fc9226a65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "error",
     "timestamp": 1747530556827,
     "user": {
      "displayName": "Riley Brodie",
      "userId": "02274550420792017755"
     },
     "user_tz": -600
    },
    "id": "a873f637-69ea-404a-b705-035fc9226a65",
    "outputId": "5d299919-7f8c-47bb-b9f4-1c1893d935ac"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7642c4e0c99f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Accuracy: {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Create subset confusion matrix\n",
    "top_N = 16\n",
    "top_class_indices = [cls for cls, _ in Counter(y_test).most_common(top_N)]\n",
    "mask = np.isin(y_test, top_class_indices)\n",
    "\n",
    "X_test_top = X_test[mask]\n",
    "y_test_top = y_test[mask]\n",
    "y_pred_top = rf_model.predict(X_test_top)\n",
    "\n",
    "cm = confusion_matrix(y_test_top, y_pred_top, labels=top_class_indices)\n",
    "class_labels = le.classes_[top_class_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(ax=ax, cmap='viridis', xticks_rotation=90)\n",
    "ax.set_title(f\"Confusion Matrix - Top {top_N} Senders\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create full confusion matrix and export to excel\n",
    "y_pred_full = rf_model.predict(X_test)\n",
    "cm_full = confusion_matrix(y_test, y_pred_full)\n",
    "cm_df = pd.DataFrame(cm_full, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "# Export to Excel\n",
    "cm_df.to_excel(\"confusion_matrix_full.xlsx\")\n",
    "print(\"Full cm exported to 'confusion_matrix_full.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fece4e9-8a45-49b0-857c-35bdf1595918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1747530548087,
     "user": {
      "displayName": "Riley Brodie",
      "userId": "02274550420792017755"
     },
     "user_tz": -600
    },
    "id": "5fece4e9-8a45-49b0-857c-35bdf1595918",
    "outputId": "d082eaad-2d67-4abc-e6e7-89ba27862e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605cda-b531-4570-b3b8-3f452e065907",
   "metadata": {
    "id": "c9605cda-b531-4570-b3b8-3f452e065907"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
