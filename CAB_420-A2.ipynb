{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eba78b-78ab-487d-b63d-f127ac7e82f9",
   "metadata": {
    "id": "87eba78b-78ab-487d-b63d-f127ac7e82f9"
   },
   "source": [
    "# CAB_420-A2: Enron Email Sender Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943258d-889d-48a1-9df0-1ee98bda2c92",
   "metadata": {
    "id": "1943258d-889d-48a1-9df0-1ee98bda2c92"
   },
   "source": [
    "# 1. Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577e02ce-521b-4689-aca2-03c8afed9912",
   "metadata": {
    "id": "577e02ce-521b-4689-aca2-03c8afed9912",
    "outputId": "77c33816-e08e-4e79-ab1f-40475f7a8331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# to set up the venv, run\n",
    "# pip install keras matplotlib seaborn scikit-learn scikit-image statsmodels opencv-python pandas gensim pillow tensorflow-datasets xgboost jupyter ipython jupyterlab openpyxl\n",
    "\n",
    "\n",
    "# Imports\n",
    "import tarfile\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b53d942e-6077-496f-9d93-938432ca469c",
   "metadata": {
    "id": "b53d942e-6077-496f-9d93-938432ca469c"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "tar_path = 'enron_mail_20150507.tar.gz'\n",
    "data_path = 'maildir'\n",
    "Email = namedtuple('Email', ['from_', 'message'])\n",
    "max_emails = 500 # Senders will stop having emails appended to their dataset once this number is reached\n",
    "min_emails = 200 # Minimum emails a class must have to be kept\n",
    "min_words = 20 # Minimum words an email needs to have to be kept\n",
    "# max_words = 200 # Max words an email needs to have to be kept\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Functions\n",
    "# Decrypts tar file\n",
    "def decrypt_tar(file_path):\n",
    "    # Decrypt tar.zg file\n",
    "    if not os.path.isdir(data_path):\n",
    "        try:\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                members = tar.getmembers()\n",
    "                for member in tqdm(members, desc=\"Extracting files\"):\n",
    "                    tar.extract(member)\n",
    "\n",
    "           # delete archive after extraction\n",
    "        except:\n",
    "            print(f\"File '{file_path}' does not exist.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Folder '{data_path}' already exists.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Adapted from https://www.youtube.com/watch?v=_hkmSGG9pyA (Pythonic Accountant on YouTube)\n",
    "# Takes all individual email files and returns them in Email objects that have referenceable sections\n",
    "def get_msg(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            from_ = ''\n",
    "            message = []\n",
    "            in_message = False\n",
    "\n",
    "            content = f.read().replace('\\r', '')\n",
    "            lines = content.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith('From:') and not from_:\n",
    "                    from_ = line.replace('From:', '').strip()\n",
    "                elif line.startswith('X-FileName:'):\n",
    "                    in_message = True\n",
    "                elif in_message:\n",
    "                    # Skip quoted replies (email forwards)\n",
    "                    if line.strip().lower().startswith('-----Original Message-----'):\n",
    "                        break\n",
    "                    message.append(line)\n",
    "\n",
    "            return Email(\n",
    "                from_=from_,\n",
    "                message=' '.join(message).strip()\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Filters out all classes with less than the minimum samples\n",
    "def filter_classes(x, y):\n",
    "    # First pass: apply filters\n",
    "    sender_counts = Counter(y)\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "\n",
    "    for (xi_raw, xi_path), yi in zip(x, y):\n",
    "        xj = preprocess(xi_raw)\n",
    "        if (\n",
    "            sender_counts[yi] >= min_emails and\n",
    "            len(xj.split()) > min_words and\n",
    "            'enron' in yi.lower()\n",
    "        ):\n",
    "            x_temp.append((xj, xi_path))\n",
    "            y_temp.append(yi)\n",
    "            \n",
    "    # Second pass: drop entries that dont fit the min-max_email thresholds\n",
    "    filtered_counts = Counter(y_temp)\n",
    "    x_filtered = []\n",
    "    y_filtered = []\n",
    "    \n",
    "    for (xi, xi_path), yi in zip(x_temp, y_temp):\n",
    "        if filtered_counts[yi] >= min_emails and filtered_counts[yi] <= max_emails:\n",
    "            x_filtered.append((xi, xi_path))\n",
    "            y_filtered.append(yi)\n",
    "\n",
    "\n",
    "\n",
    "    count_filtered = Counter(y_filtered)\n",
    "    print(f\"Filtered: {len(x_filtered)} emails from {len(set(y_filtered))} users\")\n",
    "    return x_filtered, y_filtered, count_filtered\n",
    "\n",
    "\n",
    "# Extracts all emails from the maildir file.\n",
    "def load_directory(maildir_path):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    #list of all folders commonly found in users emails that contain non-useful informaton.\n",
    "    irrelevant_folders = {'calendar', 'contacts', 'notes', 'deleted_items', 'journal'}\n",
    "    \n",
    "    for root, dirs, files in os.walk(maildir_path):\n",
    "        if any(folder in root.lower() for folder in irrelevant_folders): #This skips folders matching the names listed in the 'irrelevant_folders' variable above.\n",
    "            continue\n",
    "        for email_file in files:\n",
    "            file_path = os.path.join(root, email_file)\n",
    "\n",
    "            # Skip directories (until a file is hit)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            # Open file, parse contents, and add to x/y arrays\n",
    "            try:\n",
    "                email = get_msg(file_path)\n",
    "                # Only keep emails that have a from and message\n",
    "                if email and email.from_ and email.message:\n",
    "                    x.append((email.message, file_path))\n",
    "                    y.append(email.from_)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    count = Counter(y)\n",
    "    print('Unfiltered data loaded')\n",
    "    return(x, y, count)\n",
    "\n",
    "def hash_text(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Process email body data\n",
    "def preprocess(text):\n",
    "\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Strip out email addresses and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove common attachment artefacts\n",
    "    text = re.sub(r'(?i)\\b[\\w\\-.]+\\.(jpg|jpeg|png|pdf|docx?|xlsx?|pptx?|zip)\\b', '', text)\n",
    "    text = re.sub(r'<<[^>]+>>', '', text)\n",
    "    text = re.sub(r'\\[cid:[^\\]]+\\]', '', text)\n",
    "\n",
    "    # Remove inline headers (leaked from replies)\n",
    "    text = re.sub(r'(?i)(from|to|cc|subject|sent):\\s?.*', '', text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d5f30-56b6-45e0-bef9-0c3af82608af",
   "metadata": {
    "id": "657d5f30-56b6-45e0-bef9-0c3af82608af"
   },
   "source": [
    "# 2. Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb51ec3-d6a5-484a-a2df-0ebfeaf72cd5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4375777d2e994faca01c30d76d154946"
     ]
    },
    "id": "2eb51ec3-d6a5-484a-a2df-0ebfeaf72cd5",
    "outputId": "15c84ef1-7577-48dc-83b6-d864703b5db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'maildir' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Extract tar file\n",
    "decrypt_tar(tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323959cf-f247-48db-bf1b-b8205324517e",
   "metadata": {
    "id": "323959cf-f247-48db-bf1b-b8205324517e",
    "outputId": "5cd4d567-a983-4779-a479-8f2a3b5aadc5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered data loaded\n",
      "Filtered: 26339 emails from 86 users\n",
      "Deduplicated: 13181 emails from 86 users\n",
      "Removed 13158 duplicate emails\n",
      "\n",
      "Found 5392 duplicate email sets.\n",
      "\n",
      "--- Showing 5 duplicate email groups ---\n",
      "\n",
      "Group #1 — 2 copies\n",
      "============================================================\n",
      "  Copy 1 | Sender: veronica.espinoza@enron.com\n",
      "  Location: maildir/hyvl-d/all_documents/362_\n",
      "  ----------------------------------------\n",
      "dan, attached below are the two credit worksheets that you requested for peoples energy resources corp. for both ena and enron mw, llc. veronica\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 2 | Sender: veronica.espinoza@enron.com\n",
      "  Location: maildir/hyvl-d/gas/chicago_office/6_\n",
      "  ----------------------------------------\n",
      "dan, attached below are the two credit worksheets that you requested for peoples energy resources corp. for both ena and enron mw, llc. veronica\n",
      "  ----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n",
      "Group #2 — 4 copies\n",
      "============================================================\n",
      "  Copy 1 | Sender: mark.greenberg@enron.com\n",
      "  Location: maildir/hyvl-d/all_documents/977_\n",
      "  ----------------------------------------\n",
      "bob - thanks for the information and access id's. as i indicated in my voice mail earlier today, the standard process is to run these deals through one central point and to ensure that a review of all descriptions and information related to each respective product to be traded is undertaken by a legal dept. member. as you will note from the list of cc's, i have included legal dept. members involved in the physical power, physical gas and financial products areas. these people will need to review the product definitions/descriptions - if any - on the tradequote site, as well as any other items associated with these products. accordingly, each of these individuals also needs a password/user id for due diligence purposes. i have almost completed my review of the access agreement and the other\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 2 | Sender: mark.greenberg@enron.com\n",
      "  Location: maildir/hyvl-d/miscellaneous/truequote/1_\n",
      "  ----------------------------------------\n",
      "bob - thanks for the information and access id's. as i indicated in my voice mail earlier today, the standard process is to run these deals through one central point and to ensure that a review of all descriptions and information related to each respective product to be traded is undertaken by a legal dept. member. as you will note from the list of cc's, i have included legal dept. members involved in the physical power, physical gas and financial products areas. these people will need to review the product definitions/descriptions - if any - on the tradequote site, as well as any other items associated with these products. accordingly, each of these individuals also needs a password/user id for due diligence purposes. i have almost completed my review of the access agreement and the other\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 3 | Sender: mark.greenberg@enron.com\n",
      "  Location: maildir/taylor-m/all_documents/7841_\n",
      "  ----------------------------------------\n",
      "bob - thanks for the information and access id's. as i indicated in my voice mail earlier today, the standard process is to run these deals through one central point and to ensure that a review of all descriptions and information related to each respective product to be traded is undertaken by a legal dept. member. as you will note from the list of cc's, i have included legal dept. members involved in the physical power, physical gas and financial products areas. these people will need to review the product definitions/descriptions - if any - on the tradequote site, as well as any other items associated with these products. accordingly, each of these individuals also needs a password/user id for due diligence purposes. i have almost completed my review of the access agreement and the other\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 4 | Sender: mark.greenberg@enron.com\n",
      "  Location: maildir/jones-t/all_documents/9710_\n",
      "  ----------------------------------------\n",
      "bob - thanks for the information and access id's. as i indicated in my voice mail earlier today, the standard process is to run these deals through one central point and to ensure that a review of all descriptions and information related to each respective product to be traded is undertaken by a legal dept. member. as you will note from the list of cc's, i have included legal dept. members involved in the physical power, physical gas and financial products areas. these people will need to review the product definitions/descriptions - if any - on the tradequote site, as well as any other items associated with these products. accordingly, each of these individuals also needs a password/user id for due diligence purposes. i have almost completed my review of the access agreement and the other\n",
      "  ----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n",
      "Group #3 — 4 copies\n",
      "============================================================\n",
      "  Copy 1 | Sender: kim.ward@enron.com\n",
      "  Location: maildir/hyvl-d/all_documents/1147_\n",
      "  ----------------------------------------\n",
      "dan, el paso electric's lawyer is will garant and his number is 512-495-8832. he is the one dealing with our lt contract and is familiar with the collateral issues. matt henry is the lawyer that drafted their long term contract attached. i can get his number if you need it but i think he works at the same firm as will. let me know what i can do to help and if you have any questions. thanks again for your help. kim ---------------------- forwarded by kim ward/hou/ect on 03/29/2001 03:38 pm --------------------------- on 03/26/2001 10:47:58 am\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 2 | Sender: kim.ward@enron.com\n",
      "  Location: maildir/hyvl-d/gas/foster_ward/20_\n",
      "  ----------------------------------------\n",
      "dan, el paso electric's lawyer is will garant and his number is 512-495-8832. he is the one dealing with our lt contract and is familiar with the collateral issues. matt henry is the lawyer that drafted their long term contract attached. i can get his number if you need it but i think he works at the same firm as will. let me know what i can do to help and if you have any questions. thanks again for your help. kim ---------------------- forwarded by kim ward/hou/ect on 03/29/2001 03:38 pm --------------------------- on 03/26/2001 10:47:58 am\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 3 | Sender: kim.ward@enron.com\n",
      "  Location: maildir/ward-k/sent_items/45_\n",
      "  ----------------------------------------\n",
      "dan, el paso electric's lawyer is will garant and his number is 512-495-8832. he is the one dealing with our lt contract and is familiar with the collateral issues. matt henry is the lawyer that drafted their long term contract attached. i can get his number if you need it but i think he works at the same firm as will. let me know what i can do to help and if you have any questions. thanks again for your help. kim ---------------------- forwarded by kim ward/hou/ect on 03/29/2001 03:38 pm --------------------------- on 03/26/2001 10:47:58 am\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 4 | Sender: kim.ward@enron.com\n",
      "  Location: maildir/ward-k/_sent_mail/188_\n",
      "  ----------------------------------------\n",
      "dan, el paso electric's lawyer is will garant and his number is 512-495-8832. he is the one dealing with our lt contract and is familiar with the collateral issues. matt henry is the lawyer that drafted their long term contract attached. i can get his number if you need it but i think he works at the same firm as will. let me know what i can do to help and if you have any questions. thanks again for your help. kim ---------------------- forwarded by kim ward/hou/ect on 03/29/2001 03:38 pm --------------------------- on 03/26/2001 10:47:58 am\n",
      "  ----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n",
      "Group #4 — 2 copies\n",
      "============================================================\n",
      "  Copy 1 | Sender: stacy.dickson@enron.com\n",
      "  Location: maildir/hyvl-d/all_documents/301_\n",
      "  ----------------------------------------\n",
      "i believe you are working on the engage relationship. ----- forwarded by stacy e dickson/hou/ect on 08/30/2000 09:22 am ----- russell diamond 08/29/2000 06:15 pm\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 2 | Sender: stacy.dickson@enron.com\n",
      "  Location: maildir/hyvl-d/gas/engage/1_\n",
      "  ----------------------------------------\n",
      "i believe you are working on the engage relationship. ----- forwarded by stacy e dickson/hou/ect on 08/30/2000 09:22 am ----- russell diamond 08/29/2000 06:15 pm\n",
      "  ----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n",
      "Group #5 — 2 copies\n",
      "============================================================\n",
      "  Copy 1 | Sender: barry.tycholiz@enron.com\n",
      "  Location: maildir/hyvl-d/all_documents/504_\n",
      "  ----------------------------------------\n",
      "there is a second document that stacey was working on that is pretty much identical as the monthly firm deal but involves only gas on alliance called aos ( this is like interuptable gas, but a little different ). the confirms has been raised previously, and you should have this on file. this is the file name on the bottom of stacey's last document. o:\\common\\legal\\sdickson\\agreements\\. let me know... ps . did you send the confirm to crestar on the firm volumes. bt\n",
      "  ----------------------------------------\n",
      "\n",
      "  Copy 2 | Sender: barry.tycholiz@enron.com\n",
      "  Location: maildir/hyvl-d/gas/crestar/11_\n",
      "  ----------------------------------------\n",
      "there is a second document that stacey was working on that is pretty much identical as the monthly firm deal but involves only gas on alliance called aos ( this is like interuptable gas, but a little different ). the confirms has been raised previously, and you should have this on file. this is the file name on the bottom of stacey's last document. o:\\common\\legal\\sdickson\\agreements\\. let me know... ps . did you send the confirm to crestar on the firm volumes. bt\n",
      "  ----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "# This section may take a while, wait for print below\n",
    "x, y, count = load_directory(data_path)\n",
    "x_filtered, y_filtered, count_filtered = filter_classes(x, y)\n",
    "\n",
    "seen = set()\n",
    "dedup_x, dedup_y = [], []\n",
    "\n",
    "# Dictionary to track all emails by hash\n",
    "hash_to_emails = defaultdict(list)\n",
    "\n",
    "for (xi_text, xi_path), yi in zip(x_filtered, y_filtered):\n",
    "    h = hash_text(xi_text)\n",
    "    hash_to_emails[h].append((xi_text, yi, xi_path))\n",
    "\n",
    "    if h not in seen:\n",
    "        seen.add(h)\n",
    "        dedup_x.append((xi_text, xi_path))\n",
    "        dedup_y.append(yi)\n",
    "\n",
    "print(f\"Deduplicated: {len(dedup_x)} emails from {len(set(dedup_y))} users\")\n",
    "print(f\"Removed {len(duplicates)} duplicate emails\")\n",
    "\n",
    "# Filter out only the hashes that have duplicates (more than one sender/email)\n",
    "duplicate_groups = {h: entries for h, entries in hash_to_emails.items() if len(entries) > 1}\n",
    "\n",
    "print(f\"\\nFound {len(duplicate_groups)} duplicate email sets.\\n\")\n",
    "print(\"--- Showing 5 duplicate email groups ---\\n\")\n",
    "\n",
    "for i, (h, entries) in enumerate(duplicate_groups.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Group #{i+1} — {len(entries)} copies\")\n",
    "    print(\"=\" * 60)\n",
    "    for j, (msg, sender, path) in enumerate(entries):\n",
    "        print(f\"  Copy {j+1} | Sender: {sender}\")\n",
    "        print(f\"  Location: {path}\")\n",
    "        print(\"  \" + \"-\" * 40)\n",
    "        print(msg[:800].strip())  # Trim message\n",
    "        print(\"  \" + \"-\" * 40 + \"\\n\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "x_filtered = dedup_x\n",
    "y_filtered = dedup_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f511595-f28b-49aa-aaee-5e087a80afc5",
   "metadata": {
    "id": "d47c4bb9-8c6e-41dd-8386-35f9e70ef757",
    "outputId": "74b22731-2d14-4009-c832-d7f341d895c3"
   },
   "outputs": [],
   "source": [
    "# Analysis of filtered vs unfiltered data\n",
    "print(f\"Unfiltered: {len(x)} emails from {len(set(y))} users\")\n",
    "print(f\"Filtered:   {len(x_filtered)} emails from {len(set(y_filtered))} users with {min_emails}-{max_emails} emails\")\n",
    "\n",
    "top_threshold = 1000 # Lower = faster charts, showing less data\n",
    "# Print top filtered senders\n",
    "if top_threshold > len(set(y_filtered)):\n",
    "    print(f\"\\nFiltered senders ({len(set(y_filtered))}):\")\n",
    "else:\n",
    "    print(f\"\\nFiltered senders ({top_threshold}):\")\n",
    "for sender, email_count in count_filtered.most_common(top_threshold):\n",
    "    print(f\"{sender}: {email_count} emails\")\n",
    "\n",
    "# Extract top data for plotting\n",
    "top_unfiltered = count.most_common(top_threshold)\n",
    "top_filtered = count_filtered.most_common(top_threshold)\n",
    "\n",
    "# Separate into sender and count lists\n",
    "senders_unf, counts_unf = zip(*top_unfiltered)\n",
    "senders_filt, counts_filt = zip(*top_filtered)\n",
    "\n",
    "# Plot data\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 16), sharey=True)\n",
    "# Unfiltered plot\n",
    "ax1.bar(senders_unf[::-1], counts_unf[::-1], color='salmon')\n",
    "ax1.set_title('Top Senders (Unfiltered)')\n",
    "ax1.set_ylabel('Email Count')\n",
    "ax1.set_xticklabels([])\n",
    "# Filtered plot\n",
    "ax2.bar(senders_filt[::-1], counts_filt[::-1], color='seagreen')\n",
    "ax2.set_title('Top Senders (Filtered)')\n",
    "ax2.set_ylabel('Email Count')\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6271-7ab8-47e6-af19-c04a30ffba63",
   "metadata": {
    "id": "52de6271-7ab8-47e6-af19-c04a30ffba63",
    "outputId": "f22785d7-642f-47c2-fc6a-d0f084366f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 28290\n",
      "x_test: 9431\n",
      "x_val: 9430\n"
     ]
    }
   ],
   "source": [
    "# Encode y (senders)\n",
    "y_encoded = le.fit_transform(y_filtered)\n",
    "\n",
    "# Split Data\n",
    "# Split (train+val) vs test\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_filtered, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Split train vs val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "print(f\"x_val: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c3ec6-95a2-45e8-a45c-ec98743e4264",
   "metadata": {
    "id": "789c3ec6-95a2-45e8-a45c-ec98743e4264"
   },
   "source": [
    "## 2.1. Processing for Non-DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7403fde-5d9a-4e51-84a6-54892cad2b44",
   "metadata": {
    "id": "f7403fde-5d9a-4e51-84a6-54892cad2b44",
    "outputId": "2d7921bd-08eb-4410-f03a-e9b304484530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (28290, 88217)\n",
      "X_test: (9431, 88217)\n",
      "X_val: (9430, 88217)\n"
     ]
    }
   ],
   "source": [
    "# Extra processing for non-DL models\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_val   = vectorizer.transform(x_val)\n",
    "X_test  = vectorizer.transform(x_test)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22afb2-d8ef-44b1-b25c-0ef1f0bb864b",
   "metadata": {
    "id": "3b22afb2-d8ef-44b1-b25c-0ef1f0bb864b"
   },
   "source": [
    "## 2.2. Processing for DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995f667-ed55-4514-8e8a-3a333d89b341",
   "metadata": {
    "id": "c995f667-ed55-4514-8e8a-3a333d89b341",
    "outputId": "9d4ff8f4-835c-4bd7-e012-65db75324ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_seq: 28290\n",
      "X_test_seq: 9431\n",
      "X_val_seq: 9430\n",
      "X_train_pad: 28290\n",
      "X_test_pad: 9431\n",
      "X_val_pad: 9430\n"
     ]
    }
   ],
   "source": [
    "# Extra processing for DL models\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\", lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(x_test)\n",
    "X_val_seq   = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=300, padding='post')\n",
    "X_test_pad  = pad_sequences(X_test_seq, maxlen=300, padding='post')\n",
    "X_val_pad   = pad_sequences(X_val_seq, maxlen=300, padding='post')\n",
    "\n",
    "print(f\"X_train_seq: {len(X_train_seq)}\")\n",
    "print(f\"X_test_seq: {len(X_test_seq)}\")\n",
    "print(f\"X_val_seq: {len(X_val_seq)}\")\n",
    "print(f\"X_train_pad: {len(X_train_pad)}\")\n",
    "print(f\"X_test_pad: {len(X_test_pad)}\")\n",
    "print(f\"X_val_pad: {len(X_val_pad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311c115-8152-4a31-83cb-2e4dce56a2f0",
   "metadata": {
    "id": "6311c115-8152-4a31-83cb-2e4dce56a2f0"
   },
   "source": [
    "# 3. ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f85cf5-0102-4f00-aba9-159d23daf106",
   "metadata": {
    "id": "b0f85cf5-0102-4f00-aba9-159d23daf106"
   },
   "source": [
    "## 3.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a8c32-9b31-4a10-9541-7c6a3e5b2b31",
   "metadata": {
    "id": "3c0a8c32-9b31-4a10-9541-7c6a3e5b2b31"
   },
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "n = 100\n",
    "r = 42\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators = n, random_state = r)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "train_preds = rf_model.predict(X_train)\n",
    "val_preds = rf_model.predict(X_val)\n",
    "test_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print('model fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a873f637-69ea-404a-b705-035fc9226a65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "error",
     "timestamp": 1747530556827,
     "user": {
      "displayName": "Riley Brodie",
      "userId": "02274550420792017755"
     },
     "user_tz": -600
    },
    "id": "a873f637-69ea-404a-b705-035fc9226a65",
    "outputId": "5d299919-7f8c-47bb-b9f4-1c1893d935ac"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7642c4e0c99f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Accuracy: {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Create subset confusion matrix\n",
    "top_N = 16\n",
    "top_class_indices = [cls for cls, _ in Counter(y_test).most_common(top_N)]\n",
    "mask = np.isin(y_test, top_class_indices)\n",
    "\n",
    "X_test_top = X_test[mask]\n",
    "y_test_top = y_test[mask]\n",
    "y_pred_top = rf_model.predict(X_test_top)\n",
    "\n",
    "cm = confusion_matrix(y_test_top, y_pred_top, labels=top_class_indices)\n",
    "class_labels = le.classes_[top_class_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(ax=ax, cmap='viridis', xticks_rotation=90)\n",
    "ax.set_title(f\"Confusion Matrix - Top {top_N} Senders\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create full confusion matrix and export to excel\n",
    "y_pred_full = rf_model.predict(X_test)\n",
    "cm_full = confusion_matrix(y_test, y_pred_full)\n",
    "cm_df = pd.DataFrame(cm_full, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "# Export to Excel\n",
    "cm_df.to_excel(\"confusion_matrix_full.xlsx\")\n",
    "print(\"Full cm exported to 'confusion_matrix_full.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fece4e9-8a45-49b0-857c-35bdf1595918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1747530548087,
     "user": {
      "displayName": "Riley Brodie",
      "userId": "02274550420792017755"
     },
     "user_tz": -600
    },
    "id": "5fece4e9-8a45-49b0-857c-35bdf1595918",
    "outputId": "d082eaad-2d67-4abc-e6e7-89ba27862e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605cda-b531-4570-b3b8-3f452e065907",
   "metadata": {
    "id": "c9605cda-b531-4570-b3b8-3f452e065907"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
